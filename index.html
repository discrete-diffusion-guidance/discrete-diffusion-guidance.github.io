<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Simple Guidance Mechanisms for Discrete Diffusion Models Project Page">
  <meta property="og:title" content="Discrete Guidance"/>
  <meta property="og:description" content="Simple Guidance Mechanisms for Discrete Diffusion Models"/>
  <meta property="og:url" content="https://discrete-guidance.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/caduceus.png" /> -->
  <!-- <meta property="og:image:width" content="630"/> -->
  <!-- <meta property="og:image:height" content="630"/> -->

  <title>Simple Guidance Mechanisms for Discrete Diffusion project page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Simple Guidance Mechanisms for<br>Discrete Diffusion Models
          </h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://yair-schiff.github.io/" target="_blank">Yair Schiff<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://s-sahoo.com/" target="_blank">Subham Sekhar Sahoo<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://hao-pt.github.io/" target="_blank">Hao Phung<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.tech.cornell.edu/people/guanghan-wang/" target="_blank">Guanghan Wang<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sam-boshar-6ba68618a/" target="_blank">Sam Boshar<sup>2</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/hugo-dalla-torre-947b8a14a" target="_blank">Hugo Dalla-torre<sup>2</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/bpalmeida/" target="_blank">Bernardo P de Almeida<sup>2</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://rush-nlp.com/" target="_blank">Alexander M Rush<sup>1</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/thomas-pierrot-120a43128/" target="_blank">Thomas Pierrot<sup>2</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov<sup>1</sup></a>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>
              Cornell&nbsp;
              <img src="static/images/cornell.png" alt="Cornell" style="float:right;width:30px;height:30px;">
            </span>
            <span class="author-block">
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>
              InstaDeep&nbsp;
              <img src="static/images/instadeep-logo.png" alt="InstaDeep" style="float:right;width:30px;height:30px;">
            </span>
          </div>
          <!-- <div class="is-size-5 publication-authors">
            <sup>*</sup>Equal contribution
          </div> -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.10193" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Youtube link
              <span class="link-block">
                <a href=YOUTUBE LINK" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/kuleshov-group/discrete-diffusion-guidance" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <!-- HuggingFace link -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/kuleshov-group/udlm-675e63ab42bc757093099e1b" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p>&#129303;</p>
                  </span>
                  <span>HuggingFace</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Hero Image -->
<section class="section hero" style="padding:0px 0px;margin:0px auto;">
  <div class="container is-max-desktop">
    <img src="static/images/udlm.gif" alt="UDLM" style="width:800px;height:400px;padding:0px;margin:0px;">
  </div>
</section>

<!-- Introduction -->
<section class="section" id="Introduction">
  <div class="container is-max-desktop">
    <h2 class="title">Guidance + DiffusionðŸ‘Œ</h2>
    <div class="content is-medium">
      State-of-the-art diffusion models are the prevailing approach for generating continuous signals, such as images and audio.
      These models are made even more useful by the mechanisms of classifier-free and classifier-based guidance that enable users to better control the generated samples.      
      In contrast, for discrete data, autoregressive (AR) models are the go-to approach, but are notoriously difficult to control, given the sequential nature of their generation process that <em>'locks in'</em> tokens during denoising. 
      Diffusion models, which have a global view of a generated signal at each denoising step, as opposed to AR models which can only plan locally (i.e., one token ahead), are arguably more suitable for controlled generation.
      Finding a way to bridge the gap to AR's language modeling performance while maintaining the control mechanisms of diffusion modeling would unlock powerful and useful generative modeling tools with widespread applications, especially in scientific domains.
      <br><br>
      Recent work on discrete diffusion (e.g., <a href="https://arxiv.org/abs/2406.07524">Sahoo et al., 2024</a>) poses a potential alternative to AR language modeling.
      The improved performance of non-autoregressive language models inspires a renewed search for guidance mechanisms that, similar to the continuous domain, can unlock high-quality controllable generation for discrete data.
    </div>
  </div>
</section>
<!-- End Introduction-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="content is-medium">
        <h2 class="title">Our contributions</h2>
        <ul>
          <li>We provide simple and effective discrete classifier-based and classifier-free guidance.</li>
          <li>We introduce UDLM, a class of discrete diffusion models particularly amenable to guidance, and we derive a tightened ELBO that significantly improves their performance.</li>
          <li>Across three domains, we demonstrate that discrete guidance yields better controllable generation compared to strong AR baselines and previous diffusion guidance methods.</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<section class="section" id="Graphical Abstract"></section>
<div class="container is-max-desktop">
  <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
    <img src="static/images/graphical_abstract.png" alt="Discrete Guidance" style="width:720px;margin-top: 0px;">
    <br>
    <em>
      <b>Adapting guidance to discrete diffusion.</b>
      (Left) Models output a factorized discrete distribution for each denoised token.
      With our guidance mechanisms, we adjust these probabilities according to a guidance model -- either a conditional diffusion model in classifier-free guidance or a separately trained classifier for classifier-based guidance.
      (Right) Relative to autoregressive models, which make local predictions one token at a time, discrete diffusion models denoise the entire sequence at every iteration, allowing for more guidable outputs.
    </em>
  </h4>
  <br>
</div>
</section>


<section class="section" id="Discrete Diffusion Models">
  <div class="container is-max-desktop">    
    <h2 class="title">Discrete Diffusion Models (a brief primer)</h2>
    <div class="content is-medium">
      In diffusion, we train a parametric model \(p_\theta\) to undo corruption from latent variables \(\mathbf{z}_t\) (for \(t \in [0, 1]\)) that are produced from a fixed forward noising process defined by \(q\)&nbsp; (<a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>; <a href="https://arxiv.org/abs/1907.05600">Song & Ermon, 2019</a>; <a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>).
      Thus, starting from a sample \(\mathbf{z}_{t=1}\) from some limiting distribution \(\boldsymbol{\pi}\), we can iteratively denoise to produce latents \(\mathbf{z}_t, \mathbf{z}_s, \ldots, \mathbf{z}_{t=0}, \mathbf{x}_0\), with \(\mathbf{x}_0\) appearing to have been drawn from the true data distribution for well-trained denoising models.
      <br><br>      
      In discrete diffusion, these variables refer to one-hot vectors, i.e., \(\mathbf{x}_0, \mathbf{z}_t \in \mathcal{V}\) where \(\mathcal{V} = \{\mathbf{z} \in \{0, 1\}^N : \sum_i \mathbf{z}_i = 1\} \subset \Delta^N\), with \(\Delta^N\) being the simplex over \(N\) categories (i.e., the vocab size).
      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/discrete_diffusion.png" alt="Discrete Diffusion" style="width:720px">
        <br>
        <em>Discrete diffusion with absorbing state (top) or uniform noise (bottom) as the limiting distribution \(\boldsymbol{\pi}\).</em>
      </h4>
      <br>
      The seminal D3PM paper (<a href="https://arxiv.org/abs/2107.03006">Austin et al., 2021</a>) defined a noising process over discrete data via transition matrices \(Q_{t|s}\) whose \((i, j)^{\text{th}}\) entries correspond to the probability of transitioning from the \(i^{\text{th}}\) state at time \(s\) to the \(j^{\text{th}}\) state at time \(t\).
      This induces a Markov corruption process where we have \(q(\mathbf{z}_t | \mathbf{z}_s) = \mathrm{Cat}(\mathbf{z}_t; Q_{t|s}\mathbf{z}_s)\).
      <a href="https://arxiv.org/abs/2406.07524">Sahoo et al. (2024)</a> build off this framework to introduce specialized algorithms that are both simpler and more effective than the general D3PM framework.
      They focus on a specific class of forward processes from D3PM that can be defined as interpolations between clean data and a noisy prior \(\boldsymbol{\pi}\), and we adopt their notation below:
      $$q(\mathbf{z}_t \mid \mathbf{x}_0) = \mathrm{Cat}(\mathbf{z}_t; \alpha_t\mathbf{x}_0 + (1 âˆ’ \alpha_t)\boldsymbol{\pi}),$$
      where \(\alpha_t = \alpha(t)\) is a noise schedule monotonically decreasing in \(t\).
      Defining \(\alpha_{t|s} = \alpha_t / \alpha_s\), this class of processes admit the following posteriors
      $$q(\mathbf{z}_s | \mathbf{z}_t, \mathbf{x}_0) = \mathrm{Cat}\left(\mathbf{z}_s; \frac{[\alpha_{t|s} \mathbf{z}_t + (1 - \alpha_{t|s})\mathbf{1} \boldsymbol{\pi}^\top \mathbf{z}_t]  \odot [\alpha_s \mathbf{x}_0 + (1 - \alpha_s) \boldsymbol{\pi}]}{\alpha_t \mathbf{z}_t^\top \mathbf{x}_0  + (1 - \alpha_t)\mathbf{z}_t^\top\boldsymbol{\pi}} \right).$$
      Of note, for absorbing-state diffusion, where \(\boldsymbol{\pi} = \boldsymbol{m}\), a one-hot vector at the special \(\texttt{[MASK]}\) token index, <a href="https://arxiv.org/abs/2406.07524">Sahoo et al. (2024)</a> show that when the latent \(\mathbf{z}_t \neq \boldsymbol{m}\) then \(q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}_0) = \mathrm{Cat}(\mathbf{z}_s; \mathbf{z}_t)\), which reflects the fact that unasked tokens at time \(t\) must remain unmasked for all time \(s < t\).
      <br><br>
      Diffusion models are trained to minimize a variational upper bound (NELBO) given by:
      $$\mathbb{E}_q\Bigg[\underbrace{- \log p_\theta(\mathbf{x}_0 | \mathbf{z}_{t(0)})}_{\normalsize\begin{array}{c}\mathcal{L}_{recons}\end{array}} + \underbrace{\sum_{i=1}^T \mathrm{KL}[q(\mathbf{z}_{s(i)} | \mathbf{z}_{t(i)}, \mathbf{x}_0) \| p_\theta(\mathbf{z}_{s(i)} | \mathbf{z}_{t(i)})]}_{\normalsize\begin{array}{c}\mathcal{L}_{diff}\end{array}}\Bigg] + \underbrace{\mathrm{KL}[q(\mathbf{z}_{t(T)} | \mathbf{x}_0) \| p_\theta(\mathbf{z}_{t(T)})]}_{\normalsize \begin{array}{c}\mathcal{L}_{prior}\end{array}},$$
      where \(\mathrm{KL}\) refers to the Kullback-Leibler divergence, and the expectation is taken over the noising process.
      \(\mathcal{L}_{prior}\) refers to the prior regularization term, which is used to ensure that the final latent \(\mathbf{z}_{t(T)}\) is close to the prior distribution. 
      \(\mathcal{L}_{recons}\) is the reconstruction loss, which measures the negative log-likelihood of the clean data given the latent at time \(t(0)\).
      Finally, \(\mathcal{L}_{diff}\) is the diffusion loss, which measures the KL divergence between the noised and denoised latents.
      <br><br>
      Often we model entire sequences, and not just individual tokens, which we denote as \(\mathbf{x}_0^{(1:L)}\) and \(\mathbf{z}_t^{(1:L)}\) for a sequences clean data and latents of length \(L\), respectively.
      We assume that the forward noising process factorizes independently across tokens, so that the noising process for a sequence is the product of the noising processes for each token and that the denoising network, when conditioned on a sequence of latent variables, factorizes independently across tokens as well.      
  </div>
</section>

<section class="section" id="Why Discrete Diffusion Guidance is Hard">
  <div class="container is-max-desktop">
    <h2 class="title">Why Guidance in Discrete Diffusion is Hard</h2>
    <div class="content is-medium">
      Current guidance mechanisms for diffusion models rely on Langevin dynamics and computation of the gradient of the log-likelihood with respected to latent variables, as below:
      $$\begin{aligned}\nabla_{\mathbf{z}_s}\log(p_\theta^\gamma(\mathbf{z}_s\mid \mathbf{z}_t, y)) &= \gamma\nabla_{\mathbf{z}_s}\log(p_\phi(y\mid \mathbf{z_s})) + \nabla_{\mathbf{z}_s}\log(p_\theta(\mathbf{z_s} \mid \mathbf{z}_t)) && \text{classifier-based} \\ \nabla_{\mathbf{z}_s}\log(p_\theta^\gamma(\mathbf{z}_s\mid \mathbf{z}_t, y)) &= \gamma\nabla_{\mathbf{z}_s}\log(p_\theta(\mathbf{z}_s \mid \mathbf{z}_t, y)) + (1-\gamma)\nabla_{\mathbf{z}_s}\log(p_\theta(\mathbf{z}_s\mid\mathbf{z}_t)) && \text{classifier-free},\end{aligned}$$
      where \(\gamma\) is an inverese temperature parameter that controls the strength of the guidance, \(y\) is the desired ouput class, \(p_\theta\) is the denoising diffusion model (unconditional / conditional), and \(p_\phi\) is a separate classifier model.
      <br><br>
      The reliance on the gradient computation blocks the application of these guidance mechanisms to discrete diffusion models, as the gradient of the log-likelihood with respect to the discrete latent variables is not defined.
    </div>
  </div>
</section>

<section class="section" id="Discrete Guidance Mechanisms">
  <div class="container is-max-desktop">
    <h2 class="title">Proposed Discrete Guidance Mechanisms</h2>
    <div class="content is-medium">
      To overcome the issues of applying gradient-based guidance mechanisms to discrete diffusion models, we propose two mechanisms for discrete guidance that re-weight the probabilities assigned by the denoising model according to a \(\gamma\)-scaled guidance term.
      <h3 class="subtitle" style="font-size:1.6rem;"><em>Discrete Classifier-Free Guidance</em></h3>
        First, we can achieve classifier-free guidance through a simple derivation that relies on (repeated) applications of Bayes' rule to the tempered distribution \(p_\theta^\gamma(\mathbf{z}_s\mid \mathbf{z}_t, y)\).
        We train a conditional and unconditonal diffusion model, both of which factorize independently across tokens in a sequence in order to sample each token independently from the following distribution:
        $$p_\theta^\gamma(\mathbf{z}_s\mid \mathbf{z}_t, y) \propto p_\theta(\mathbf{z}_s\mid \mathbf{z}_t, y)^\gamma\cdot p_\theta(\mathbf{z}_s \mid \mathbf{z}_t)^{(1-\gamma)}.$$
        That is, we sample each token from a weighted combination of the conditional and unconditional diffusion models.
        Since both \(p_\theta(\mathbf{z}_s\mid \mathbf{z}_t, y)\) and \(p_\theta(\mathbf{z}_s \mid \mathbf{z}_t)\) are modeled with independent factorization across tokens, we can sample an entire sequence as follows:
        $$p^\gamma_\theta(\mathbf{z}_s^{(1:L)} \mid _t^{(1:L)}, y) = \prod_{\ell=1}^{L}\frac{1}{Z^{(\ell)}}p_\theta(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)}, y)^\gamma p_\theta(\mathbf{z}_s^{(\ell)}\mid\mathbf{z}_t^{(1:L)})^{(1-\gamma)},$$ 
        where \(Z^{(\ell)} = \sum_{\mathbf{z}'_{s}}p_\theta(\mathbf{z}'_{s} \mid \mathbf{z}_t^{(1:L)}, y)^\gamma p_\theta(\mathbf{z}'_s\mid\mathbf{z}_t^{(1:L)})^{(1-\gamma)}\) is the per-token partition function.
      <br><br>
        We dub this guidance mechanism as <b>D-CFG</b> for <b>D</b>iscerete <b>C</b>lassifier-<b>F</b>ree <b>G</b>uidance.
      <br><br>
      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight: normal;">
        <img src="static/images/discrete_guidance.png" alt="Discrete Guidance" style="width:720px;margin-top: 0px;">
        <br>
        <em>
          The temperature \(\gamma\) controls the strength of guidance, i.e., how strongly we 'focus' on the conditional distribution.
          (Left) <b>Discrete Classifier-Free Guidance</b>&nbsp;&nbsp;We downweigh the output of an unconditional diffusion model and upweigh the output of conditional diffusion model (both of which factorize independently across a sequence) to produce a guided distribution per token.
          (Right) <b>Discrete Classifier-Based Guidance</b>&nbsp;&nbsp;We adjust the output of the unconditional model by the probability assigned by a classifier to sequences with a single token change.
          Our assumption that the guided distribution factorizes independently across tokens allows us to tractably compute this distribution.
        </em>
      </h4>
    
      <h3 class="subtitle" style="font-size:1.6rem;"><em>Discrete Classifier-Based Guidance</em></h3>
        Extending classifier-based guidance to diffusion models is difficult because the guiding classifier need not factorize the same as the diffusion denoising network, which would imply that the classifier needs to be evaluated on exponentially many sequence combinations.
        We resolve this using factorization assumptions on the decoding model and a Taylor expansion trick.
        <br><br>
        To formulate discrete classifier-based guidance, we make the assumption that conditioned on \(\mathbf{z}_t^{(1:L)}\), the tempered distribution from which we want to sample \(p^\gamma(\mathbf{z}_s^{(1:L)} \mid \mathbf{z}_t^{1:L}, y)\) factorizes independently across tokens.
        Therefore, we can focus on the tempered distirbution of each token \(\mathbf{z}_s^{(\ell)},\) for \(\ell \in 1,\ldots, L\):
        $$p^\gamma(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)}, y) \propto p(y \mid \mathbf{z}_s^{(\ell)}, \mathbf{z}_t^{(1:L)})^\gamma p(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)}).$$
        In practice, we can sample from \(p^\gamma(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)}, y)\) by training a classifier on noised latents \(\mathbf{z}_t^{(1:L)}\) for \(t \in [0, 1]\) and use this to model the first term on the right hand side by only evaluating \(p_\phi\) on sequences for which \(\mathbf{z}_s^{(1:L)}\) and \(\mathbf{z}_t^{(1:L)}\) differ by at most the token at position \(\ell\).
        We will define this set of sequences as \(\tilde{\mathcal{Z}}_\ell(\mathbf{z_t}^{1:L}) = \{\tilde{\mathbf{z}}^{(1:L)} \mid \tilde{\mathbf{z}}^{(\ell')} = \mathbf{z_t}^{(\ell')}\text{ for all }\ell' \neq \ell\}\).
        We can then sample from the re-normalized distribution:
        $$p^\gamma_{\phi, \theta}(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)}, y) = \frac{p_\phi(y \mid \tilde{\mathbf{z}}^{(1:L)})^\gamma p_\theta(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)})}{\sum_{\tilde{\mathbf{z}}^{(1:L)}}p_\phi(y \mid \tilde{\mathbf{z}}^{(1:L)})^\gamma p_\theta(\mathbf{z}_s^{(\ell)} \mid \mathbf{z}_t^{(1:L)})}.$$
        Restricting the summation in the denominator of the equation above to \(\tilde{\mathcal{Z}}_\ell(\mathbf{z}_t^{(1:L)})\) makes normalization tractable.
        <br><br>
        Our method can be thought of as an adaptation of the successful FUDGE (<a href="https://arxiv.org/abs/2104.05218">Yang & Klein, 2021</a>) approach, which guides AR generation, to discrete diffusion, similar to how NOS (<a href="https://arxiv.org/abs/2305.20009">Gruver et al., 2024</a>) extended the AR guidance mechanism of PPLM (<a href="https://arxiv.org/abs/1912.02164">Dathathri et al., 2019</a>) to diffusion models.
        We dub this guidance mechanism as <b>D-CBG</b> for <b>D</b>iscerete <b>C</b>lassifier-<b>B</b>ased <b>G</b>uidance.
    </div> 
  </div>
</section>

<section class="section" id="UDLM">
  <div class="container is-max-desktop">
    <h2 class="title">Uniform Diffusion Language Models (UDLM)</h2>
    <div class="content is-medium">
      While masked diffusion models demonstrate better language modeling compared to other discrete diffusion (<a href="https://arxiv.org/abs/2107.03006">Austin et al. 2021</a>; <a href="https://arxiv.org/abs/2310.16834">Lou et al. 2023</a>), we argue that they are less amenable to guidance, since once a token is unmasked at some time \(t\) it remains so for all \(s < t\).
      In contrast, with uniform noising, intermediate latents can be refined multiple times throughout the denoising process.
      We therefore revisit categorical uniform noise discrete diffusion, where \(\boldsymbol{\pi} = \boldsymbol{u} = 1/N\), where \(N\) is the size of the vocabulary.
      Our aim is that by analyzing this class of diffusion models more carefully, we can reduce the gap to absorbing-state and yield performant models that are more easily steered by the guidance tools we developed above.
      <br><br>
      <b>Uniform Noise Forward Process</b>&nbsp&nbsp
      We formulate uniform noise diffusion using the interpolating discrete diffusion framework (<a href="https://arxiv.org/abs/2406.07524">Sahoo et al. (2024)</a>).
      When letting \(\boldsymbol{\pi} = \boldsymbol{u}\), the input \(\mathbf{x}\) transitions to a random state with some probability at each time step. 
      Crucially, after \(\mathbf{x}\) changes once, it can do so again.
      Formally, when \(\boldsymbol{\pi} = \boldsymbol{u}\), the posterior from above becomes
      $$q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}_0) = \mathrm{Cat} \left(\mathbf{z}_s;\frac{N \alpha_t \mathbf{z}_t \odot \mathbf{x}_0 + (\alpha_ts - \alpha_t)\mathbf{z}_t + (\alpha_s - \alpha_t)\mathbf{x}_0 + \frac{(\alpha_s - \alpha_t)(1- \alpha_s)}{N \alpha_s}\boldsymbol{1}}{N \alpha_t\langle \mathbf{z}_t, \mathbf{x}_0\rangle  + 1 - \alpha_t}\right)$$

      <b>Denoising Process</b>&nbsp&nbsp The optimal form for the reverse diffusion process \(p_\theta\) matches the posterior.
      In fact, setting \(p_\theta\) to the posterior reduces the KL terms in the ELBO to zero.
      However, setting \(p_\theta\) to exactly the posterior is not possible because it cannot be a function \(\mathbf{x}_0\) (which \(p_\theta\) is generating).
      Therefore, we introduce a predictive model of the 'clean' data given a noisy latent \(\mathbf{z}_t\) at time \(t\).
      We use \(\mathbf{x}_\theta\) to parameterize the denoising process as \(p_\theta(\mathbf{z}_s \mid \mathbf{z}_t) = q(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x} = \mathbf{x}_\theta)\), yielding:
      $$p_\theta(\mathbf{z}_s \mid \mathbf{z}_t) = \mathrm{Cat} \left(\mathbf{z}_s;\frac{N\alpha_t \mathbf{z}_t \odot \mathbf{x}_\theta + (\alpha_ts - \alpha_t)\mathbf{z}_t + (\alpha_s - \alpha_t)\mathbf{x}_\theta + \frac{(\alpha_s - \alpha_t)(1- \alpha_s)}{N\alpha_s}\boldsymbol{1}}{N \alpha_t\langle \mathbf{z}_t, \mathbf{x}_\theta\rangle  + 1 - \alpha_t}\right).$$
      Note that this minimizes the \(\mathcal{L}_{diff}\) precisely when \(\mathbf{x}_\theta = \mathbf{x}_0,\) as desired.
      <br><br>
      <b>UDLM</b>&nbsp&nbsp To build towards <b>U</b>niform <b>D</b>iscrete <b>L</b>anguage <b>M</b>odels (UDLM), we derive an improved variational objective by taking \(T\rightarrow\infty\) and analyzing each term \(\mathcal{L}_{recons}, \mathcal{L}_{diff}, \mathcal{L}_{prior}\), introduced above.
      This yields three improvements: (1) a simple and elegant closed-form expression for the variational bound that is easier to reason about; (2) an analytical reduction of \(\mathcal{L}_{recons}, \mathcal{L}_{prior}\) to zero, which tightens the ELBO; (3) a further tightening via the continuous-time extension of \(\mathcal{L}_{diff}\).
      Please refer to <a href="https://arxiv.org/abs/2412.10193" target="_blank">our manuscript</a> for more details about this derivation.
    </div>
  </div>
</section>

<!-- Experiments -->
<section class="section" id="Experiments">
  <div class="container is-max-desktop">
    <h2 class="title">Experiments</h2>
    <div class="content is-medium">
      <h3 class="subtitle" style="font-size:1.6rem;"><em>Language Modeling Results</em></h3>
      Our language modeling experiments reveal that (1) contrary to a widely-held belief, <b>uniform noise diffusion can attain state-of-the-art performance</b> on small vocabulary datasets , and that (2) our <b>UDLM is state-of-the-art among uniform noise diffusion models</b>.      
      <br><br>
      <table>
        <caption style="font-size:1.1rem;font-weight:normal"><em>UDLM performs best in smaller vocabulary regimes. Values correspond to perplexity (PPL; \(\downarrow\)) on various datasets. Best values are <b>bolded.</b></em></caption>
        <tr>
          <th>Domain</th>
          <th>Dataset</th>
          <th>Vocab. size</th>
          <th>AR</th>
          <th>MDLM</th>
          <th>UDLM</th>
        </tr>
        <tr>
          <td style="border:none"><i>Bio</i></td>
          <td style="border:none">Species-10</td>
          <td style="border:none">12</td>
          <td style="border:none"><strong>2.88</strong></td>
          <td style="border:none">3.17\(_{\geq}\)</td>
          <td style="border:none">3.15\(_{\geq}\)</td>
        </tr>
        <tr>
          <td style="border:none"><i>Chem</i></td>
          <td style="border:none">QM9</td>
          <td style="border:none">40</td>
          <td style="border:none">2.19</td>
          <td style="border:none">2.12\(_{\geq}\)</td>
          <td style="border:none"><strong>2.02</strong>\(_{\geq}\)</td>
        </tr>
        <tr>
          <td style="border:none"><i>Images</i></td>
          <td style="border:none">CIFAR-10</td>
          <td style="border:none">256</td>
          <td style="border:none">-</td>
          <td style="border:none"><strong>9.14</strong>\(_{\geq}\)</td>
          <td style="border:none">11.21\(_{\geq}\)</td>
        </tr>
        <tr>
          <td style="border:none"><i>NLP</i></td>
          <td style="border:none">text8</td>
          <td style="border:none">35</td>
          <td style="border:none"><strong>2.35</strong></td>
          <td style="border:none">2.62\(_{\geq}\)</td>
          <td style="border:none">2.71\(_{\geq}\)</td>
        </tr>
        <tr>
          <td style="border:none"></td>
          <td style="border:none">Amazon</td>
          <td style="border:none">30,522</td>
          <td style="border:none"><strong>21.67</strong></td>
          <td style="border:none">24.93\(_{\geq}\)</td>
          <td style="border:none">27.27\(_{\geq}\)</td>
        </tr>
        <tr>
          <td></td>
          <td>LM1B</td>
          <td>30,522</td>
          <td><strong>22.32</strong></td>
          <td>27.04\(_{\geq}\)</td>
          <td>31.28\(_{\geq}\)</td>
        </tr>
        <tr></tr>
      </table>

      <br><br>
      <table>
        <caption style="font-size:1.1rem;font-weight:normal"><em>UDLM outperforms previously reported uniform noise discrete diffusion models on natural language text datasets. Best values are <b>bolded</b>.</em></caption>
        <tr>
          <th>Dataset</th>
          <th>D3PM Uniform</th>
          <th>SEDD Uniform</th>
          <th>UDLM</th>
        </tr>
        <tr>
          <td style="border: none;">text8 (BPC \(\downarrow\))</td>
          <td style="border: none;">1.61</td>
          <td style="border: none;">1.47</td>
          <td style="border: none;"><strong>1.44</strong></td>
        </tr>
        <tr>
          <td>LM1B (PPL \(\downarrow\))</td>
          <td>77.59</td>
          <td>40.25</td>
          <td><strong>31.28</strong></td>
        </tr>
        <tr></tr>
      </table>

      <h3 class="subtitle" style="font-size:1.6rem;"><em>Guidance Results</em></h3>
      Our guidance results indicate that (1) <b>classifier-free guidance is more useful when paired with diffusion models compared to AR</b> and that (2) <b>our proposed D-CBG is the best classifier-based method</b> for discrete guidance, especially when combined with UDLM.
      <br><br>
      
      <b>Species-specific Genome Generation:</b>&nbsp;&nbsp;
      We train on the reference genomes of ten diverese species using sequences of 32,768 base pairs.
      We then generate novel sequences using the trained models conditioned on the species label.
      As quality measures for each species class, we compute the Jensen-Shannon (JS) distance between the \(k\)-mer frequencies of the generated sequences and those from the validation set, and report the mean JS across species (weighted by species frequency in the dataset), with smaller values indicating better \(k\)-mer distributional overlap between the ground truth and generated sequences.
      Additionally, we train a small classifier to distinguish between generated and validation set sequences and report the area under the receiver operator curve for this classifier (Disc. AUROC).
      Values closer to 0.5 indicate that the classifier is unable to distinguish between synthetic and true sequences.
      To measure the controllability, we train a separate classifier on this dataset and measure macro F1 score of this oracle classifier on the generated sequences.
      For reference, we also provide metrics for randomly generating sequences with nucleotide frequencies proportional to species representation in the data.
      <br><br>
      We find that both MDLM and UDLM are able to better generate sequences that match the desired control parameter, with higher F1 scores relative to AR.
      Moreover, UDLM is able to outperform MDLM in satisfying this control.
      Importantly, we find that only UDLM is amenable to increasing the guidance parameter \(\gamma\), where its metrics improves while AR and MDLM metrics degrade.
      Finally, of note, the diffusion model generation for this experiment is accomplished with far fewer function evaluations compared to AR.
      Whereas AR must decode each of the 32,768 tokens, because MDLM and UDLM can decode multiple tokens in parallel, we generate with \(T = 512\).
      <br><br>

      <table>
        <caption style="font-size:1.1rem;font-weight:normal"><em>Diffusion decoding with D-CFG is more controllable than AR for genomic sequences.
          Best values are <b>bolded</b>.</em></caption>
        <tr>
          <th>Model</th>
          <th>Guidance</th>
          <th>3-mer JS (\(\downarrow\))</th>
          <th>6-mer JS (\(\downarrow\))</th>
          <th>Disc. AUROC (\(\uparrow\))</th>
          <th>Oracle F1 (\(\uparrow\))</th>
        </tr>
        <tr>
          <td style="border: none;">Random</td>
          <td style="border: none;">N/A</td>
          <td style="border: none;">0.13</td>
          <td style="border: none;">0.22</td>
          <td style="border: none;">1.00</td>
          <td style="border: none;">0.07</td>
        </tr>
        <tr>
          <td style="border: none;">AR</td>
          <td style="border: none;">D-CFG\(_{\gamma=1}\)</td>
          <td style="border: none;">0.03</td>
          <td style="border: none;">0.07</td>
          <td style="border: none;">0.53</td>
          <td style="border: none;">0.87</td>
        </tr>
        <tr>
          <td style="border: none;">AR</td>
          <td style="border: none;">D-CFG\(_{\gamma=2}\)</td>
          <td style="border: none;">0.05</td>
          <td style="border: none;">0.12</td>
          <td style="border: none;">0.90</td>
          <td style="border: none;">0.81</td>
        </tr>
        <tr>
          <td style="border: none;">AR</td>
          <td style="border: none;">D-CFG\(_{\gamma=3}\)</td>
          <td style="border: none;">0.07</td>
          <td style="border: none;">0.15</td>
          <td style="border: none;">0.97</td>
          <td style="border: none;">0.74</td>
        </tr>
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">D-CFG\(_{\gamma=1}\)</td>
          <td style="border: none;"><strong> 0.02</strong></td>
          <td style="border: none;"><strong> 0.06</strong></td>
          <td style="border: none;"><strong>0.51</strong></td>
          <td style="border: none;">0.88</td>
        </tr>
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">D-CFG\(_{\gamma=2}\)</td>
          <td style="border: none;">0.05</td>
          <td style="border: none;">0.11</td>
          <td style="border: none;">0.74</td>
          <td style="border: none;">0.91</td>
        </tr>
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">D-CFG\(_{\gamma=3}\)</td>
          <td style="border: none;">0.11</td>
          <td style="border: none;">0.20</td>
          <td style="border: none;">0.93</td>
          <td style="border: none;">0.78</td>
        </tr>
        <tr>
          <td style="border: none;">UDLM</td>
          <td style="border: none;">D-CFG\(_{\gamma=1}\)</td>
          <td style="border: none;"><strong> 0.02</strong></td>
          <td style="border: none;"><strong> 0.06</strong></td>
          <td style="border: none;">0.52</td>
          <td style="border: none;">0.91</td>
        </tr>
        <tr>
          <td style="border: none;">UDLM</td>
          <td style="border: none;">D-CFG\(_{\gamma=2}\)</td>
          <td style="border: none;">0.05</td>
          <td style="border: none;">0.13</td>
          <td style="border: none;">0.61</td>
          <td style="border: none;">0.93</td>
        </tr>
        <tr>
          <td>UDLM</td>
          <td>D-CFG\(_{\gamma=3}\)</td>
          <td>0.08</td>
          <td>0.20</td>
          <td>0.87</td>
          <td><strong>0.94</strong></td>
        </tr>
        <tr></tr>
      </table>
      <br><br>

      <b>Molecule Property Maximization:</b>&nbsp;&nbsp;
      We use the small molecule QM9 dataset to investigate novel generation of sequences that maximize some property, either drug-likeness (QED) or a count of the number of rings present in the molecule.
      Our goal is to explore which models and guidance mechanisms best trade-off sample quality and control.
      We only report values for which we generated at least 50 novel sequences (out of 1,024).
      In the figure below, we see that our guidance mechanisms enable diffusion generation that better trades-off sample novelty with guidance property satisfaction.
      <br><br>
      <h4 class="subtitle has-text-centered" style="font-size:1.1rem;font-weight:normal">
        <img src="static/images/qed_cbg.png" alt="D-CBG" style="width:360px">
        <img src="static/images/ring_count_cfg.png" alt="D-CFG" style="width:360px">
        <br>
        <em>Diffusion models extend the steer-ability Pareto frontier compared to AR.
        (Left) Comparing D-CBG and FUDGE for maximizing the drug-likeness (QED) property in the QM9 dataset. 
        Using our D-CBG outperforms FUDGE classifier guidance for AR.
        <!-- We display \(\gamma\) values for which 50 or more of the generated sequences are novel. -->
        Only UDLM can accommodate larger \(\gamma\) values that enable better molecule property maximization.
        (Right) Comparing D-CFG for diffusion and AR models for maximizing ring-count in the QM9 dataset.
        Diffusion models better trade-off novel generation and property maximization.</em>
      </h4>
      <br><br>
      
      <b>Class-conditional Image Generation:</b>&nbsp;&nbsp;
      We train discrete diffusion models on a discretized version of the CIFAR10 dataset.
      We then conditionally generate images (with and without classifier-free guidance).
      In the tables below on the left, we see that both MDLM and UDLM outperform finite-time counterparts (in the form of D3PM) with improved image quality metrics of Frechet inception distance (FID) and Inception Score (IS).
      This is especially true when we add guidance using D-CFG.
      Additionally, on the right table, we explore faster inference settings (smaller \(T\) ) where diffusion models predict multiple pixels in parallel.
      MDLM's performance deteriorates with smaller \(T\), whereas UDLM is robust to this setting.
      This validates a key motivation behind UDLM: in settings where MDLM 'locks in' certain predictions that it cannot change, UDLM is more resilient given that all tokens can change throughout the decoding process.
      <br><br>
      <table style="float: left;width: 460px;margin-right: 45px">
        <caption style="font-size:1.1rem;font-weight:normal"><em>Guidance improves quality on discretized CIFAR10. FID and IS for finite- (D3PM) and continuous-time (MDLM / UDLM) discrete diffusion models. Guidance using D-CFG (\(\gamma=4\)).
          Best values are <b>bolded</b>.</em></caption>
        <tr>
          <th>Model</th>
          <th>FID (\(\downarrow\))</th>
          <th>IS (\(\uparrow\))</th>
        </tr>
        <tr>
          <td style="border: none;">D3PM Absorb</td>
          <td style="border: none;">41.28</td>
          <td style="border: none;">6.26</td>
        </tr>
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">33.75</td>
          <td style="border: none;">6.74</td>
        </tr>
        <tr>
          <td>MDLM D-CFG</td>
          <td><strong>15.56</strong></td>
          <td><strong>9.02</strong></td>
        </tr>
        <tr></tr>
        <tr>
          <td style="border: none;">D3PM Uniform</td>
          <td style="border: none;">51.27</td>
          <td style="border: none;">5.99</td>
        </tr>
        <tr>
          <td style="border: none;">UDLM</td>
          <td style="border: none;">33.65</td>
          <td style="border: none;">6.86</td>
        </tr>
        <tr>
          <td>UDLM D-CFG</td>
          <td>23.21</td>
          <td>8.66</td>
        </tr>
        <tr></tr>
      </table>
      <table style="float: rirght;width: 445px">
        <caption style="font-size:1.1rem;font-weight:normal"><em>UDLM is robust to faster sampling.
          Images sampled from a conditional model (D-CFG\(_{\gamma=1}\)) trained on CIFAR10.
          F1 metric from a separate classifier trained to identify class label used for generation.
          Best metric per \(T\) is <b>bolded</b>.</em></caption>
        <tr>
          <th>Model</th>
          <th>FID (\(\downarrow\))</th>
          <th>IS (\(\uparrow\))</th>
          <th>F1 (\(\uparrow\))</th>
        </tr>
        <tr>
          <td style="border: none;colspan: 100">\(T=128\)</td>
        </tr> 
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">64.09</td>
          <td style="border: none;">5.81</td>
          <td style="border: none;">0.63</td>
        </tr>
        <tr>
          <td>UDLM</td>
          <td><strong>30.48</strong></td>
          <td><strong>7.30</strong></td>
          <td><strong>0.80</strong></td>
        </tr>
        <tr>
          <td style="border: none;colspan: 100">\(T=1024\)</td>
        </tr>
        <tr>
          <td style="border: none;">MDLM</td>
          <td style="border: none;">27.94</td>
          <td style="border: none;">7.13</td>
          <td style="border: none;"><strong>0.81</strong></td>
        </tr>
        <tr>
          <td>UDLM</td>
          <td><strong>26.70</strong></td>
          <td><strong>7.43</strong></td>
          <td><strong>0.81</strong></td>
        </tr>
        <tr></tr> 
      </table>
    </div>
  </div>
  <!-- End Experiments-->

<!-- Conclusion -->
<section class="section" id="Conclusion">
  <div class="container is-max-desktop">
    <h2 class="title">Conclusion</h2>
    <div class="content is-medium">
      In search of a more controllable diffusion process, in this work, we derived a tight variational bound for uniform noise discrete diffusion, closing the gap to state-of-the-art absorbing-state diffusion models.
      We also highlighted that contrary to previous findings, in small vocabulary regimes, uniform noise is on par or better than absorbing state.
      We then demonstrated that straightforward adaptations of classifier-based and classifier-free guidance can offer improved guided generation relative to AR models.
      We found that with classifier-free mechanisms, diffusion models are more amenable to control without sacrificing quality of generated sequences.
      We also demonstrated that our classifier-based method is better than previous ones for both AR and diffusion models.
    </div>
  </div>
<!-- End Conclusion-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{schiff2024discreteguidance,
          title={Simple Guidance Mechanisms for Discrete Diffusion Models},          
          author={Schiff, Yair and Sahoo, Subham Sekhar and Phung, Hao and Wang, Guanghan and Boshar, Sam and Dalla-torre, Hugo and de Almeida, Bernardo P and Rush, Alexander and Pierrot, Thomas and Kuleshov, Volodymyr},
          journal={arXiv preprint arXiv:2412.10193},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content is-medium">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
